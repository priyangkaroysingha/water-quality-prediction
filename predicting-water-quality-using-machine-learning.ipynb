{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project\n",
    "\n",
    "- **ppm:** parts per million\n",
    "- **μg/L:** microgram per litre\n",
    "- **mg/L:** milligram per litre\n",
    "\n",
    "**Data Dictionary:**\n",
    "\n",
    "1. **ph:** pH of 1. water (0 to 14).\n",
    "2. **Hardness:** Capacity of water to precipitate soap in mg/L.\n",
    "3. **Solids:** Total dissolved solids in ppm.\n",
    "4. **Chloramines:** Amount of Chloramines in ppm.\n",
    "5. **Sulfate:** Amount of Sulfates dissolved in mg/L.\n",
    "6. **Conductivity:** Electrical conductivity of water in μS/cm.\n",
    "7. **Organic_carbon:** Amount of organic carbon in ppm.\n",
    "8. **Trihalomethanes:** Amount of Trihalomethanes in μg/L.\n",
    "9. **Turbidity:** Measure of light emiting property of water in NTU.\n",
    "10. **Potability:** Indicates if water is safe for human consumption. Potable -1 and Not potable -0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install xgboost\n",
    "# !pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\users\\subro\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in c:\\users\\subro\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from imbalanced-learn) (2.2.1)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in c:\\users\\subro\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from imbalanced-learn) (1.15.1)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in c:\\users\\subro\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from imbalanced-learn) (1.6.1)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in c:\\users\\subro\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from imbalanced-learn) (0.1.3)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in c:\\users\\subro\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in c:\\users\\subro\\appdata\\local\\programs\\python\\python313\\lib\\site-packages (from imbalanced-learn) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install imbalanced-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-6b1459d9af09>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimblearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'imblearn'"
     ]
    }
   ],
   "source": [
    "import imblearn\n",
    "print(imblearn.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV,StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, RocCurveDisplay\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "\n",
    "# Set Seaborn Style\n",
    "sns.set(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load Data\n",
    "def load_data(filepath):\n",
    "    \"\"\"\n",
    "    Load dataset from a CSV file.\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): Path to the dataset.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Loaded dataset as a pandas DataFrame.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the CSV file into a pandas DataFrame\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"Data loaded successfully from {filepath}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {filepath}\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while loading the file: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Handle Missing Values\n",
    "def handle_missing_values(df):\n",
    "    \"\"\"\n",
    "    Handle missing values in the dataset using SimpleImputer.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataset.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataset with missing values imputed.\n",
    "    \"\"\"\n",
    "    imputer = SimpleImputer(strategy='mean')  # Impute missing values with the column mean\n",
    "    df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns)\n",
    "    \n",
    "    # Display summary of missing values after imputation\n",
    "    print(\"Missing values after imputation:\")\n",
    "    print(df_imputed.isnull().sum())\n",
    "    \n",
    "    return df_imputed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Encode Categorical Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Encode Categorical Variables\n",
    "def encode_categorical(df, columns):\n",
    "    \"\"\"\n",
    "    Encode categorical columns into numerical values using Label Encoding.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataset.\n",
    "        columns (list): List of column names to encode.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Dataset with encoded categorical columns.\n",
    "    \"\"\"\n",
    "    for col in columns:\n",
    "        if col in df.columns:\n",
    "            le = LabelEncoder()\n",
    "            df[col] = le.fit_transform(df[col].astype(str))  # Ensure all values are strings\n",
    "            print(f\"Encoded column: {col}\")\n",
    "        else:\n",
    "            print(f\"Warning: Column '{col}' not found in dataset.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Scale Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Scale Features\n",
    "def scale_features(df, target_column='Potability'):\n",
    "    \"\"\"\n",
    "    Scale numerical features in the dataset while excluding the target column.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataset.\n",
    "        target_column (str): Name of the target column to exclude from scaling (default is 'Potability').\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Scaled dataset.\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    features = df.drop(columns=[target_column])\n",
    "    \n",
    "    # Initialize the scaler\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Scale the features\n",
    "    scaled_features = scaler.fit_transform(features)\n",
    "    \n",
    "    # Convert scaled features back to DataFrame\n",
    "    scaled_df = pd.DataFrame(scaled_features, columns=features.columns)\n",
    "    \n",
    "    # Add the target column back to the dataset\n",
    "    scaled_df[target_column] = df[target_column].values\n",
    "    \n",
    "    return scaled_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Split Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Split Data\n",
    "def split_data(df, target_column='Potability', test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Split dataset into training and testing sets.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): Input dataset.\n",
    "        target_column (str): Name of the target column (default is 'Potability').\n",
    "        test_size (float): Proportion of the dataset to include in the test split (default is 0.2).\n",
    "        random_state (int): Random state for reproducibility (default is 42).\n",
    "    \n",
    "    Returns:\n",
    "        tuple: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    # Ensure target column exists in the dataset\n",
    "    if target_column not in df.columns:\n",
    "        raise ValueError(f\"Target column '{target_column}' not found in the dataset.\")\n",
    "\n",
    "    # Separate features (X) and target (y)\n",
    "    X = df.drop(columns=[target_column])\n",
    "    y = df[target_column]\n",
    "\n",
    "    # Split data into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=random_state)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Data Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Data Preprocessing Pipeline\n",
    "def preprocess_pipeline(filepath, target_column, categorical_columns=None, test_size=0.2, random_state=42):\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline.\n",
    "    \n",
    "    Args:\n",
    "        filepath (str): Path to the dataset.\n",
    "        target_column (str): Name of the target column.\n",
    "        categorical_columns (list): List of categorical columns (optional).\n",
    "        test_size (float): Proportion of the dataset to include in the test split.\n",
    "        random_state (int): Random state for reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: X_train, X_test, y_train, y_test\n",
    "    \"\"\"\n",
    "    # Load data\n",
    "    df = load_data(filepath)\n",
    "    if df is None:\n",
    "        return None, None, None, None\n",
    "\n",
    "    # Handle missing values\n",
    "    df = handle_missing_values(df)\n",
    "\n",
    "    # Encode categorical columns (if any)\n",
    "    if categorical_columns:\n",
    "        df = encode_categorical(df, categorical_columns)\n",
    "\n",
    "    # Scale features\n",
    "    X_scaled = scale_features(df, target_column)\n",
    "\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = split_data(\n",
    "        X_scaled,\n",
    "        target_column,\n",
    "        test_size,\n",
    "        random_state,\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7. Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Model Training and Evaluation\n",
    "def train_evaluate_model(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train and evaluate a Random Forest Classifier on the given dataset.\n",
    "    \n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Training features.\n",
    "        X_test (pd.DataFrame): Testing features.\n",
    "        y_train (pd.Series): Training labels.\n",
    "        y_test (pd.Series): Testing labels.\n",
    "    \n",
    "    Returns:\n",
    "        RandomForestClassifier: Trained model.\n",
    "    \"\"\"\n",
    "    # Initialize the Random Forest Classifier\n",
    "    model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Model Accuracy: {accuracy:.2f}\\n\")\n",
    "    \n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=[\"Not Potable\", \"Potable\"], yticklabels=[\"Not Potable\", \"Potable\"])\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.show()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Script\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify parameters\n",
    "    filepath = r\"C:\\Pinky_Folder\\github_folder\\water-quality-prediction\\data\\water_potability.csv\"\n",
    "    target_column = \"Potability\"\n",
    "    categorical_columns = []  # List the categorical columns here if needed\n",
    "    \n",
    "    # Preprocess data\n",
    "    X_train, X_test, y_train, y_test = preprocess_pipeline(filepath, target_column, categorical_columns)\n",
    "\n",
    "    # Ensure data is properly loaded and preprocessed\n",
    "    if X_train is not None:\n",
    "        # Print dataset shapes\n",
    "        print(\"Training Features Shape:\", X_train.shape)\n",
    "        print(\"Testing Features Shape:\", X_test.shape)\n",
    "        print(\"Training Labels Shape:\", y_train.shape)\n",
    "        print(\"Testing Labels Shape:\", y_test.shape)\n",
    "\n",
    "        # Visualize class distribution\n",
    "        sns.countplot(x=y_train)\n",
    "        plt.title(\"Class Distribution in Training Set\")\n",
    "        plt.xlabel(\"Potability Labels\")\n",
    "        plt.ylabel(\"Count\")\n",
    "        plt.show()\n",
    "\n",
    "        # Train and evaluate model\n",
    "        trained_model = train_evaluate_model(X_train, X_test, y_train, y_test)\n",
    "    else:\n",
    "        print(\"Data preprocessing failed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Observations:**\n",
    "- The model achieved an accuracy of 68%, which is decent but shows room for improvement, especially in predicting the minority class (label 1.0). Here's a breakdown of the classification metrics:\n",
    "\n",
    "*Precision:*\n",
    "- For 0.0: 0.70 (70% of predicted 0.0 are correct).\n",
    "- For 1.0: 0.61 (61% of predicted 1.0 are correct).\n",
    "- Indicates the model is better at identifying 0.0 correctly.\n",
    "\n",
    "*Recall:*\n",
    "- For 0.0: 0.86 (86% of actual 0.0 are identified correctly).\n",
    "- For 1.0: 0.38 (only 38% of actual 1.0 are identified correctly).\n",
    "- Indicates the model struggles to identify the minority class (1.0).\n",
    "\n",
    "*F1-Score:*\n",
    "- Combines precision and recall. For 1.0, it’s 0.47, which reflects poor balance for this class.\n",
    "\n",
    "*Class Imbalance:*\n",
    "- The support (number of samples) is **imbalanced:** 412 for 0.0 vs. 244 for 1.0. The imbalance skews the model towards predicting the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendations to Improve the Model:\n",
    "Handle Class Imbalance:\n",
    "- Use techniques like SMOTE (Synthetic Minority Over-sampling Technique) or ADASYN to oversample the minority class.\n",
    "Alternatively, try undersampling the majority class.\n",
    "\n",
    "Hyperparameter Tuning:\n",
    "- Adjust parameters like n_estimators, max_depth, and class_weight in the Random Forest model.\n",
    "- Setting class_weight='balanced' can help address the class imbalance.\n",
    "\n",
    "Feature Engineering:\n",
    "- Explore feature correlations to identify redundant or uninformative features.\n",
    "- Create new features that may improve class separation.\n",
    "\n",
    "Use a Different Algorithm:\n",
    "- Try gradient-boosted algorithms like XGBoost, LightGBM, or CatBoost, which often handle class imbalance better.\n",
    "\n",
    "Cross-Validation:\n",
    "- Perform k-fold cross-validation to ensure the model generalizes well across all data splits.\n",
    "\n",
    "Evaluate Additional Metrics:\n",
    "- Use ROC-AUC to evaluate the model’s performance, especially for imbalanced datasets.\n",
    "\n",
    "Analyze Errors:\n",
    "- Examine misclassified samples to understand where the model struggles and adjust preprocessing or features accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. SMOTE and Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. SMOTE and Hyperparameter tuning\n",
    "def train_and_evaluate(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"\n",
    "    Train and evaluate a Random Forest classifier with SMOTE for class balancing.\n",
    "    \n",
    "    Args:\n",
    "        X_train (pd.DataFrame): Training features.\n",
    "        X_test (pd.DataFrame): Testing features.\n",
    "        y_train (pd.Series): Training labels.\n",
    "        y_test (pd.Series): Testing labels.\n",
    "    \"\"\"\n",
    "    # Apply SMOTE to balance the classes in the training set\n",
    "    smote = SMOTE(random_state=42)\n",
    "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "    print(\"Class distribution after SMOTE:\")\n",
    "    print(pd.Series(y_train_balanced).value_counts())\n",
    "    \n",
    "    # Define a Random Forest model\n",
    "    rf = RandomForestClassifier(random_state=42, class_weight='balanced')\n",
    "    \n",
    "    # Hyperparameter tuning using GridSearchCV\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [10, 20, None],\n",
    "        'min_samples_split': [2, 5],\n",
    "    }\n",
    "    grid_search = GridSearchCV(rf, param_grid, scoring='roc_auc', cv=5, n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train_balanced, y_train_balanced)\n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    print(f\"Best Parameters: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(\"\\nModel Evaluation:\")\n",
    "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "    print(\"ROC-AUC:\", roc_auc_score(y_test, y_pred_proba))\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    \n",
    "    # Plot ROC Curve\n",
    "    RocCurveDisplay.from_estimator(best_model, X_test, y_test)\n",
    "    plt.title(\"ROC Curve\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main Script\n",
    "if __name__ == \"__main__\":\n",
    "    # Specify parameters\n",
    "    filepath = r\"C:\\Pinky_Folder\\github_folder\\water-quality-prediction\\data\\water_potability.csv\"\n",
    "    target_column = \"Potability\"\n",
    "    categorical_columns = []  # Update if there are categorical columns\n",
    "    \n",
    "    # Preprocess data\n",
    "    X_train, X_test, y_train, y_test = preprocess_pipeline(filepath, target_column, categorical_columns)\n",
    "    \n",
    "    # Ensure data is properly loaded and preprocessed\n",
    "    if X_train is not None:\n",
    "        # Train and evaluate the model\n",
    "        train_and_evaluate(X_train, X_test, y_train, y_test)\n",
    "    else:\n",
    "        print(\"Data preprocessing failed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Improvement\n",
    "- Applies SMOTE to handle class imbalance in the training set.\n",
    "- Tunes Hyperparameters for XGBoost using RandomizedSearchCV and stratified cross-validation.\n",
    "- Evaluates the Model using accuracy, ROC-AUC, and a detailed classification report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize XGBoost Classifier\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\", random_state=42)\n",
    "\n",
    "# Define parameter grid for tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200, 300],\n",
    "    'max_depth': [3, 5, 7, 9, None],\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'subsample': [0.6, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.8, 1.0],\n",
    "    'gamma': [0, 1, 5],\n",
    "    'min_child_weight': [1, 5, 10],\n",
    "}\n",
    "\n",
    "# Apply SMOTE to the training data\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Perform Stratified Cross-Validation with RandomizedSearchCV\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=xgb_model,\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=30,\n",
    "    scoring='roc_auc',\n",
    "    cv=cv,\n",
    "    verbose=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "random_search.fit(X_train_smote, y_train_smote)\n",
    "\n",
    "# Get the best parameters and evaluate on test data\n",
    "best_xgb = random_search.best_estimator_\n",
    "print(\"Best Parameters:\", random_search.best_params_)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = best_xgb.predict(X_test)\n",
    "y_pred_prob = best_xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"\\nModel Evaluation:\")\n",
    "print(\"Accuracy:\", best_xgb.score(X_test, y_test))\n",
    "print(\"ROC-AUC:\", roc_auc_score(y_test, y_pred_prob))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance:\n",
    "- Accuracy: 65.1% is moderate but not ideal for classification.\n",
    "- ROC-AUC: 69.6% indicates the model's ability to distinguish between classes is fair but can be improved.\n",
    "- Classification Report:\n",
    "- The model performs better on class 0.0 than class 1.0. This could be due to an imbalance in the dataset even after using SMOTE or limitations in the model's ability to learn minority class patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Deployment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.1 Save the Trained Model\n",
    "- Save the trained XGBoost model to disk using Python's joblib or pickle libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "model_path = \"xgboost_model.pkl\"\n",
    "joblib.dump(best_xgb, model_path)\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10.2 Web App"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!streamlit run app.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
